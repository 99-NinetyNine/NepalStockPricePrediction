{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8a8e15e9-7781-4198-b4f1-06e8427ffdae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#entiment anaylssi\n",
    "import pandas as pd\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e4ac7662-51ad-4464-bbe2-79a4a9d0735a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Heading</th>\n",
       "      <th>Link to detail</th>\n",
       "      <th>Detail News</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023-09-26</td>\n",
       "      <td>Banks and Finance Companies Report Impressive ...</td>\n",
       "      <td>https://www.sharesansar.com/newsdetail/banks-a...</td>\n",
       "      <td>Nepal Rastra Bank has unveiled recent statisti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023-09-18</td>\n",
       "      <td>Nabil Bank inaugurates Namche Branch in Solukh...</td>\n",
       "      <td>https://www.sharesansar.com/newsdetail/nabil-b...</td>\n",
       "      <td>Nabil Bank, a leading banking institution, has...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023-09-18</td>\n",
       "      <td>Commercial Banks in Ashwin Unveil Interest Rat...</td>\n",
       "      <td>https://www.sharesansar.com/newsdetail/commerc...</td>\n",
       "      <td>Commercial banks have recently announced their...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2023-09-12</td>\n",
       "      <td>Nabil Bank partners with Sipradi Autoparts to ...</td>\n",
       "      <td>https://www.sharesansar.com/newsdetail/nabil-b...</td>\n",
       "      <td>Nabil Bank has inked an agreement with Sipradi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2023-09-11</td>\n",
       "      <td>Commercial Banks; Too Pricey on your Pockets? ...</td>\n",
       "      <td>https://www.sharesansar.com/newsdetail/commerc...</td>\n",
       "      <td>As the stock market experiences continuous dec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2023-09-04</td>\n",
       "      <td>Nabil Bank introduces Special Teej Offer for f...</td>\n",
       "      <td>https://www.sharesansar.com/newsdetail/nabil-b...</td>\n",
       "      <td>Targeting to approaching Teej festival – one o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2023-09-01</td>\n",
       "      <td>Nabil Bank bags Ambitions and Development Asia...</td>\n",
       "      <td>https://www.sharesansar.com/newsdetail/nabil-b...</td>\n",
       "      <td>Nabil Bank has bagged ‘Ambitions and Developme...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2023-08-28</td>\n",
       "      <td>Nabil Bank receives Sales Excellence Award 202...</td>\n",
       "      <td>https://www.sharesansar.com/newsdetail/nabil-b...</td>\n",
       "      <td>Nabil Bank has received the Sales Excellence A...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2023-08-14</td>\n",
       "      <td>Nabil Bank introduces Nabil Dhukka Ghar Karja ...</td>\n",
       "      <td>https://www.sharesansar.com/newsdetail/nabil-b...</td>\n",
       "      <td>With an aim to fulfill the dream of owning hou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2023-08-08</td>\n",
       "      <td>Comparative Analysis: Unraveling Financial Ind...</td>\n",
       "      <td>https://www.sharesansar.com/newsdetail/compara...</td>\n",
       "      <td>The fourth-quarter reports for the ongoing fis...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date                                            Heading  \\\n",
       "0  2023-09-26  Banks and Finance Companies Report Impressive ...   \n",
       "1  2023-09-18  Nabil Bank inaugurates Namche Branch in Solukh...   \n",
       "2  2023-09-18  Commercial Banks in Ashwin Unveil Interest Rat...   \n",
       "3  2023-09-12  Nabil Bank partners with Sipradi Autoparts to ...   \n",
       "4  2023-09-11  Commercial Banks; Too Pricey on your Pockets? ...   \n",
       "5  2023-09-04  Nabil Bank introduces Special Teej Offer for f...   \n",
       "6  2023-09-01  Nabil Bank bags Ambitions and Development Asia...   \n",
       "7  2023-08-28  Nabil Bank receives Sales Excellence Award 202...   \n",
       "8  2023-08-14  Nabil Bank introduces Nabil Dhukka Ghar Karja ...   \n",
       "9  2023-08-08  Comparative Analysis: Unraveling Financial Ind...   \n",
       "\n",
       "                                      Link to detail  \\\n",
       "0  https://www.sharesansar.com/newsdetail/banks-a...   \n",
       "1  https://www.sharesansar.com/newsdetail/nabil-b...   \n",
       "2  https://www.sharesansar.com/newsdetail/commerc...   \n",
       "3  https://www.sharesansar.com/newsdetail/nabil-b...   \n",
       "4  https://www.sharesansar.com/newsdetail/commerc...   \n",
       "5  https://www.sharesansar.com/newsdetail/nabil-b...   \n",
       "6  https://www.sharesansar.com/newsdetail/nabil-b...   \n",
       "7  https://www.sharesansar.com/newsdetail/nabil-b...   \n",
       "8  https://www.sharesansar.com/newsdetail/nabil-b...   \n",
       "9  https://www.sharesansar.com/newsdetail/compara...   \n",
       "\n",
       "                                         Detail News  \n",
       "0  Nepal Rastra Bank has unveiled recent statisti...  \n",
       "1  Nabil Bank, a leading banking institution, has...  \n",
       "2  Commercial banks have recently announced their...  \n",
       "3  Nabil Bank has inked an agreement with Sipradi...  \n",
       "4  As the stock market experiences continuous dec...  \n",
       "5  Targeting to approaching Teej festival – one o...  \n",
       "6  Nabil Bank has bagged ‘Ambitions and Developme...  \n",
       "7  Nabil Bank has received the Sales Excellence A...  \n",
       "8  With an aim to fulfill the dream of owning hou...  \n",
       "9  The fourth-quarter reports for the ongoing fis...  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.read_csv(\"nice_news\")\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "3938cc69-8f47-4a56-9b53-cffc054b5b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "t=df[\"Detail News\"][:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cea9a9db-6b38-4f9d-b44e-083f46a08bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7d0b942-7a55-4641-a5cd-9b74f3ffb946",
   "metadata": {},
   "outputs": [],
   "source": [
    "##pipeline of text preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ba2b1938-2328-4fde-9779-121b4c74589c",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'float' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[36], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m l\u001b[38;5;241m=\u001b[39m[]\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sentence \u001b[38;5;129;01min\u001b[39;00m t:\n\u001b[0;32m----> 4\u001b[0m     vs \u001b[38;5;241m=\u001b[39m \u001b[43manalyzer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpolarity_scores\u001b[49m\u001b[43m(\u001b[49m\u001b[43msentence\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m     l\u001b[38;5;241m.\u001b[39mappend([vs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mneg\u001b[39m\u001b[38;5;124m\"\u001b[39m],vs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mneu\u001b[39m\u001b[38;5;124m\"\u001b[39m],vs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpos\u001b[39m\u001b[38;5;124m\"\u001b[39m],vs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompound\u001b[39m\u001b[38;5;124m\"\u001b[39m],])\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/vaderSentiment/vaderSentiment.py:241\u001b[0m, in \u001b[0;36mSentimentIntensityAnalyzer.polarity_scores\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    239\u001b[0m text_no_emoji \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    240\u001b[0m prev_space \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 241\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m \u001b[38;5;28mchr\u001b[39m \u001b[38;5;129;01min\u001b[39;00m text:\n\u001b[1;32m    242\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mchr\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39memojis:\n\u001b[1;32m    243\u001b[0m         \u001b[38;5;66;03m# get the textual description\u001b[39;00m\n\u001b[1;32m    244\u001b[0m         description \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39memojis[\u001b[38;5;28mchr\u001b[39m]\n",
      "\u001b[0;31mTypeError\u001b[0m: 'float' object is not iterable"
     ]
    }
   ],
   "source": [
    "analyzer = SentimentIntensityAnalyzer()\n",
    "l=[]\n",
    "for sentence in t:\n",
    "    vs = analyzer.polarity_scores(sentence)\n",
    "    l.append([vs[\"neg\"],vs[\"neu\"],vs[\"pos\"],vs[\"compound\"],])\n",
    "df_x=pd.DataFrame(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c318b780-7c89-4c89-b37c-176a288d2956",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "802639cc-9f0b-4f24-9b37-d1d3bda32648",
   "metadata": {},
   "outputs": [],
   "source": [
    "import textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6e4b55c4-63b2-4b71-aac1-296bbeddbf09",
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "s=[]\n",
    "for text in t:    \n",
    "    analysis = TextBlob(text)\n",
    "    sentiment = analysis.sentiment\n",
    "    s.append([sentiment.polarity,sentiment.subjectivity])\n",
    "df1=pd.DataFrame(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1855b2d5-88ad-45ee-8bf7-3ce190ebbdf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/sailesh/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
      "[nltk_data] Downloading package punkt to /home/sailesh/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3b2f7d97-f26b-4c29-8f63-3e79034125f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dff=pd.concat([df_x,df1],axis=1)\n",
    "dff.columns=[\"vader +ve\",\"vader -ve\", \"vader neutral\", \"textblob polarity\",\"textblob_subjectivity\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9a09f1ba-480a-48f7-97b5-56073b302446",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "043293e5-e143-4757-946f-a237bc8ad7ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/sailesh/nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download(\"wordnet\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0096154a-5dab-43fe-ad33-8b9bfded355f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text:\n",
      "This is a sample sentence. It 213 232 454% includes punctuation and stop words like 'the' and 'is'.\n",
      "\n",
      "Cleaned Text:\n",
      "sample sentence 213 232 454 includes punctuation stop words like 'the 'is\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Tokenization\n",
    "tokens = word_tokenize(text)\n",
    "\n",
    "# Remove punctuation\n",
    "tokens = [word for word in tokens if word not in string.punctuation]\n",
    "\n",
    "# Convert to lowercase\n",
    "tokens = [word.lower() for word in tokens]\n",
    "\n",
    "# Remove stop words\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "tokens = [word for word in tokens if word not in stop_words]\n",
    "\n",
    "import re\n",
    "\n",
    "# Join tokens back into a clean text\n",
    "clean_text = \" \".join(tokens)\n",
    "clean_text = re.sub(r'\\d+', 'NUM', clean_text)  # Replace numbers with 'NUM'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70707392-d41a-4b34-b9c7-8e9104bb337f",
   "metadata": {},
   "outputs": [],
   "source": [
    "##heading analysis\n",
    "h=df[\"Heading\"][:10]\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "l=[]\n",
    "for sentence in t:\n",
    "    vs = analyzer.polarity_scores(sentence)\n",
    "    l.append([vs[\"neg\"],vs[\"neu\"],vs[\"pos\"],vs[\"compound\"],])\n",
    "df_x=pd.DataFrame(l)\n",
    "\n",
    "from textblob import TextBlob\n",
    "\n",
    "s=[]\n",
    "for text in t:    \n",
    "    analysis = TextBlob(text)\n",
    "    sentiment = analysis.sentiment\n",
    "    s.append([sentiment.polarity,sentiment.subjectivity])\n",
    "df1=pd.DataFrame(s)\n",
    "\n",
    "df_heading=pd.concat([df_x,df1],axis=1)\n",
    "df_heading.columns=[\"vader +ve\",\"vader -ve\", \"vader neutral\", \"textblob polarity\",\"textblob_subjectivity\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9411511a-0e1f-43e7-aa3c-8c83d3b7c7b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_heading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "92a33e26-ff96-40fe-83d0-10af74fc611e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n",
      "229\n",
      "230\n",
      "231\n",
      "232\n",
      "233\n",
      "234\n",
      "235\n",
      "236\n",
      "237\n",
      "238\n",
      "239\n",
      "240\n",
      "241\n",
      "242\n",
      "243\n",
      "244\n",
      "245\n",
      "246\n",
      "247\n",
      "248\n",
      "249\n",
      "250\n",
      "251\n",
      "252\n",
      "253\n",
      "254\n",
      "255\n",
      "256\n",
      "257\n",
      "258\n",
      "259\n",
      "260\n",
      "261\n",
      "262\n",
      "263\n",
      "264\n",
      "265\n",
      "266\n",
      "267\n",
      "268\n",
      "269\n",
      "270\n",
      "271\n",
      "272\n",
      "273\n",
      "274\n",
      "275\n",
      "276\n",
      "277\n",
      "278\n",
      "279\n",
      "280\n",
      "281\n",
      "282\n",
      "283\n",
      "284\n",
      "285\n",
      "286\n",
      "287\n",
      "288\n",
      "289\n",
      "290\n",
      "291\n",
      "292\n",
      "293\n",
      "294\n",
      "295\n",
      "296\n",
      "297\n",
      "298\n",
      "299\n",
      "300\n",
      "301\n",
      "302\n",
      "303\n",
      "304\n",
      "305\n",
      "306\n",
      "307\n",
      "308\n",
      "309\n",
      "310\n",
      "311\n",
      "312\n",
      "313\n",
      "314\n",
      "315\n",
      "316\n",
      "317\n",
      "318\n",
      "319\n",
      "320\n",
      "321\n",
      "322\n",
      "323\n",
      "324\n",
      "325\n",
      "326\n",
      "327\n",
      "328\n",
      "329\n",
      "330\n",
      "331\n",
      "332\n",
      "333\n",
      "334\n",
      "335\n",
      "336\n",
      "337\n",
      "338\n",
      "339\n",
      "340\n",
      "341\n",
      "342\n",
      "343\n",
      "344\n",
      "345\n",
      "346\n",
      "347\n",
      "348\n",
      "349\n",
      "350\n",
      "351\n",
      "352\n",
      "353\n",
      "354\n",
      "355\n",
      "356\n",
      "357\n",
      "358\n",
      "359\n",
      "360\n",
      "361\n",
      "362\n",
      "363\n",
      "364\n",
      "365\n",
      "366\n",
      "367\n",
      "368\n",
      "369\n",
      "370\n",
      "371\n",
      "372\n",
      "373\n",
      "374\n",
      "375\n",
      "376\n",
      "377\n",
      "378\n",
      "379\n",
      "380\n",
      "381\n",
      "382\n",
      "383\n",
      "384\n",
      "385\n",
      "386\n",
      "387\n",
      "388\n",
      "389\n",
      "390\n",
      "391\n",
      "392\n",
      "393\n",
      "394\n",
      "395\n",
      "396\n",
      "397\n",
      "398\n",
      "399\n",
      "400\n",
      "401\n",
      "402\n",
      "403\n",
      "404\n",
      "405\n",
      "406\n",
      "407\n",
      "408\n",
      "409\n",
      "410\n",
      "411\n",
      "412\n",
      "413\n",
      "414\n",
      "415\n",
      "416\n",
      "417\n",
      "418\n",
      "419\n",
      "420\n",
      "421\n",
      "422\n",
      "423\n",
      "424\n",
      "425\n",
      "426\n",
      "427\n",
      "428\n",
      "429\n",
      "430\n",
      "431\n",
      "432\n",
      "433\n",
      "434\n",
      "435\n",
      "436\n",
      "437\n",
      "438\n",
      "439\n",
      "440\n",
      "441\n",
      "442\n",
      "443\n",
      "444\n",
      "445\n",
      "446\n",
      "447\n",
      "448\n",
      "449\n",
      "450\n",
      "451\n",
      "452\n",
      "453\n",
      "454\n",
      "455\n",
      "456\n",
      "457\n",
      "458\n",
      "459\n",
      "460\n",
      "461\n",
      "462\n",
      "463\n",
      "464\n",
      "465\n",
      "466\n",
      "467\n",
      "468\n",
      "469\n",
      "470\n",
      "471\n",
      "472\n",
      "473\n",
      "474\n",
      "475\n",
      "476\n",
      "477\n",
      "478\n",
      "479\n",
      "480\n",
      "481\n",
      "482\n",
      "483\n",
      "484\n",
      "485\n",
      "486\n",
      "487\n",
      "488\n",
      "489\n",
      "490\n",
      "491\n",
      "492\n",
      "493\n",
      "494\n",
      "495\n",
      "496\n",
      "497\n",
      "498\n",
      "499\n",
      "500\n",
      "501\n",
      "502\n",
      "503\n",
      "504\n",
      "505\n",
      "506\n",
      "507\n",
      "508\n",
      "509\n",
      "510\n",
      "511\n",
      "512\n",
      "513\n",
      "514\n",
      "515\n",
      "516\n",
      "517\n",
      "518\n",
      "519\n",
      "520\n",
      "521\n",
      "522\n",
      "523\n",
      "524\n",
      "525\n",
      "526\n",
      "527\n",
      "528\n",
      "529\n",
      "530\n",
      "531\n",
      "532\n",
      "533\n",
      "534\n",
      "535\n",
      "536\n",
      "537\n",
      "538\n",
      "539\n",
      "540\n",
      "541\n",
      "542\n",
      "543\n",
      "544\n",
      "545\n",
      "546\n",
      "547\n",
      "548\n",
      "549\n",
      "550\n",
      "551\n",
      "552\n",
      "553\n",
      "554\n",
      "555\n",
      "556\n",
      "557\n",
      "558\n",
      "559\n",
      "560\n",
      "561\n",
      "562\n",
      "563\n",
      "564\n",
      "565\n",
      "566\n",
      "567\n",
      "568\n",
      "569\n",
      "570\n",
      "571\n",
      "572\n",
      "573\n",
      "574\n",
      "575\n",
      "576\n",
      "577\n",
      "578\n",
      "579\n",
      "580\n",
      "581\n",
      "582\n",
      "583\n",
      "584\n",
      "585\n",
      "586\n",
      "587\n",
      "588\n",
      "589\n",
      "590\n",
      "591\n",
      "592\n",
      "593\n",
      "594\n",
      "595\n",
      "596\n",
      "597\n",
      "598\n",
      "599\n",
      "600\n",
      "601\n",
      "602\n",
      "603\n",
      "604\n",
      "605\n",
      "606\n",
      "607\n",
      "608\n",
      "609\n",
      "610\n",
      "611\n",
      "612\n",
      "613\n",
      "614\n",
      "615\n",
      "616\n",
      "617\n",
      "618\n",
      "619\n",
      "620\n",
      "621\n",
      "622\n",
      "623\n",
      "624\n",
      "625\n",
      "626\n",
      "627\n",
      "628\n",
      "629\n",
      "630\n",
      "631\n",
      "632\n",
      "633\n",
      "634\n",
      "635\n",
      "636\n",
      "637\n",
      "638\n",
      "639\n",
      "640\n",
      "641\n",
      "642\n",
      "643\n",
      "644\n",
      "645\n",
      "646\n",
      "647\n",
      "648\n",
      "649\n",
      "650\n",
      "651\n",
      "652\n",
      "653\n",
      "654\n",
      "655\n",
      "656\n",
      "657\n",
      "658\n",
      "659\n",
      "660\n",
      "661\n",
      "662\n",
      "663\n",
      "664\n",
      "665\n",
      "666\n",
      "667\n",
      "668\n",
      "669\n",
      "670\n",
      "671\n",
      "672\n",
      "673\n",
      "674\n",
      "675\n",
      "676\n",
      "677\n",
      "678\n",
      "679\n",
      "680\n",
      "681\n",
      "682\n",
      "683\n",
      "684\n",
      "685\n",
      "686\n",
      "687\n",
      "688\n",
      "689\n",
      "690\n",
      "691\n",
      "692\n",
      "693\n",
      "694\n",
      "695\n",
      "696\n",
      "697\n",
      "698\n",
      "699\n",
      "700\n",
      "701\n",
      "702\n",
      "703\n",
      "704\n",
      "705\n",
      "706\n",
      "707\n",
      "708\n",
      "709\n",
      "710\n",
      "711\n",
      "712\n",
      "713\n",
      "714\n",
      "715\n",
      "716\n",
      "717\n",
      "718\n",
      "719\n",
      "720\n",
      "721\n",
      "722\n",
      "723\n",
      "724\n",
      "725\n",
      "726\n",
      "727\n",
      "728\n",
      "729\n",
      "730\n",
      "731\n",
      "732\n",
      "733\n",
      "734\n",
      "735\n",
      "736\n",
      "737\n",
      "738\n",
      "739\n",
      "740\n",
      "741\n",
      "742\n",
      "743\n",
      "744\n",
      "745\n",
      "746\n",
      "747\n",
      "748\n",
      "749\n",
      "750\n",
      "751\n",
      "752\n",
      "753\n",
      "754\n",
      "755\n",
      "756\n",
      "757\n",
      "758\n",
      "759\n",
      "760\n",
      "761\n",
      "762\n",
      "763\n",
      "764\n",
      "765\n",
      "766\n",
      "767\n",
      "768\n",
      "769\n",
      "770\n",
      "771\n",
      "772\n",
      "773\n",
      "774\n",
      "775\n",
      "776\n",
      "777\n",
      "778\n",
      "779\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from textblob import TextBlob\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.tag import pos_tag\n",
    "\n",
    "# Function to convert numbers and percentages to words\n",
    "\n",
    "\n",
    "# Function to extend abbreviations\n",
    "def extend_abbreviations(text):\n",
    "    # Replace abbreviations with full forms (you can add more as needed)\n",
    "    abbreviations = {\n",
    "            'i.e.': 'that is',\n",
    "            'e.g.': 'for example',\n",
    "            'etc.': 'and so on',\n",
    "            'Mr.': 'Mister',\n",
    "            'Mrs.': 'Missus',\n",
    "            'Dr.': 'Doctor',\n",
    "            'Ave.': 'Avenue',\n",
    "            'St.': 'Street',\n",
    "            'Apt.': 'Apartment',\n",
    "            'Jan.': 'January',\n",
    "            'Feb.': 'February',\n",
    "            'Mar.': 'March',\n",
    "            'Apr.': 'April',\n",
    "            'Aug.': 'August',\n",
    "            'Sept.': 'September',\n",
    "            'Oct.': 'October',\n",
    "            'Nov.': 'November',\n",
    "            'Dec.': 'December',\n",
    "            'AM': 'Ante Meridiem',\n",
    "            'PM': 'Post Meridiem',\n",
    "            'that\\'s': 'that is',\n",
    "            'it\\'s': 'it is'\n",
    "    }\n",
    "        \n",
    "\n",
    "    for abbr, full_form in abbreviations.items():\n",
    "        text = text.replace(abbr, full_form)\n",
    "    return text\n",
    "\n",
    "# Function to perform POS tagging and remove prepositions and conjunctions\n",
    "def pos_tagging_and_remove_stopwords(text):\n",
    "    # Tokenize the text\n",
    "    words = word_tokenize(text)\n",
    "    \n",
    "    # Perform POS tagging\n",
    "    tagged_words = pos_tag(words)\n",
    "    \n",
    "    # Define the list of POS tags for prepositions and conjunctions\n",
    "    # You can customize this list as needed\n",
    "    unwanted_tags = ['IN', 'CC']\n",
    "    \n",
    "    # Filter out words with unwanted POS tags\n",
    "    filtered_words = [word for word, tag in tagged_words if tag not in unwanted_tags]\n",
    "    \n",
    "    # Join the remaining words back into a sentence\n",
    "    return ' '.join(filtered_words)\n",
    "\n",
    "# Function to perform lemmatization\n",
    "def lemmatize_text(text):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    # Function to map Treebank POS tags to WordNet POS tags\n",
    "    def get_wordnet_pos(treebank_tag):\n",
    "        if treebank_tag.startswith('J'):\n",
    "            return wordnet.ADJ\n",
    "        elif treebank_tag.startswith('V'):\n",
    "            return wordnet.VERB\n",
    "        elif treebank_tag.startswith('N'):\n",
    "            return wordnet.NOUN\n",
    "        elif treebank_tag.startswith('R'):\n",
    "            return wordnet.ADV\n",
    "        else:\n",
    "            return wordnet.NOUN  # Default to noun\n",
    "    \n",
    "    # Tokenize and perform POS tagging\n",
    "    words = word_tokenize(text)\n",
    "    tagged_words = pos_tag(words)\n",
    "    \n",
    "    # Lemmatize using WordNet POS tags\n",
    "    lemmatized_words = [lemmatizer.lemmatize(word, get_wordnet_pos(tag)) for word, tag in tagged_words]\n",
    "    \n",
    "    # Join the lemmatized words back into a sentence\n",
    "    return ' '.join(lemmatized_words)\n",
    "\n",
    "# Function to analyze sentiment using TextBlob\n",
    "def analyze_sentiment(text):\n",
    "    blob = TextBlob(text)\n",
    "    polarity = blob.sentiment.polarity\n",
    "    subjectivity = blob.sentiment.subjectivity\n",
    "    return polarity, subjectivity\n",
    "\n",
    "# Sample text\n",
    "l=[]\n",
    "i=0\n",
    "for text in t:\n",
    "    # Task 1: Convert numbers and percentages to words\n",
    "    text = number2word(text)\n",
    "    \n",
    "    # Task 2: Extend abbreviations\n",
    "    text = extend_abbreviations(text)\n",
    "    \n",
    "    # Task 3: Perform POS tagging and remove prepositions and conjunctions\n",
    "    text = pos_tagging_and_remove_stopwords(text)\n",
    "    \n",
    "    # Task 4: Remove stop words (optional, you can do this before POS tagging as well)\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    words = word_tokenize(text)\n",
    "    filtered_words = [word for word in words if word.lower() not in stop_words]\n",
    "    text = ' '.join(filtered_words)\n",
    "    \n",
    "    # Task 5: Lemmatize the text\n",
    "    text = lemmatize_text(text)\n",
    "    \n",
    "    # Task 6: Analyze sentiment using TextBlob\n",
    "    polarity, subjectivity = analyze_sentiment(text)\n",
    "    print(i)\n",
    "    i+=1\n",
    "    l.append([polarity, subjectivity])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "2dd35930-85e7-48be-b267-23cad4cd4678",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import inflect\n",
    "\n",
    "# Function to convert numbers and percentages to words\n",
    "def number2word(text):\n",
    "    try:\n",
    "        # Initialize the inflect engine\n",
    "        p = inflect.engine()\n",
    "        \n",
    "        # Convert numbers to words\n",
    "        text = re.sub(r'\\b(\\d+)\\b', lambda x: p.number_to_words(int(x.group(0))), text)\n",
    "        \n",
    "        # Convert percentages to words\n",
    "        text = re.sub(r'\\b(\\d+\\.\\d+)%\\b', lambda x: p.number_to_words(float(x.group(0))) + ' percent', text)\n",
    "    except:\n",
    "        text=\"\"\n",
    "    return text\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "9f29714f-4341-4332-a7dd-407ea42ce23e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment=pd.DataFrame(l,columns=[\"polarity\", \"subjectivity\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "ec982ef2-abed-4ffe-87c4-3620f40d2e5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "780"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "d681fced-bcf3-4c43-9236-4785a3accebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df=pd.concat([df,sentiment],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "e02a88e6-ea47-4aa2-bc00-f9760ec570d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Heading</th>\n",
       "      <th>Link to detail</th>\n",
       "      <th>Detail News</th>\n",
       "      <th>polarity</th>\n",
       "      <th>subjectivity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023-09-26</td>\n",
       "      <td>Banks and Finance Companies Report Impressive ...</td>\n",
       "      <td>https://www.sharesansar.com/newsdetail/banks-a...</td>\n",
       "      <td>Nepal Rastra Bank has unveiled recent statisti...</td>\n",
       "      <td>0.100940</td>\n",
       "      <td>0.227429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023-09-18</td>\n",
       "      <td>Nabil Bank inaugurates Namche Branch in Solukh...</td>\n",
       "      <td>https://www.sharesansar.com/newsdetail/nabil-b...</td>\n",
       "      <td>Nabil Bank, a leading banking institution, has...</td>\n",
       "      <td>0.202564</td>\n",
       "      <td>0.410256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023-09-18</td>\n",
       "      <td>Commercial Banks in Ashwin Unveil Interest Rat...</td>\n",
       "      <td>https://www.sharesansar.com/newsdetail/commerc...</td>\n",
       "      <td>Commercial banks have recently announced their...</td>\n",
       "      <td>0.007362</td>\n",
       "      <td>0.271507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2023-09-12</td>\n",
       "      <td>Nabil Bank partners with Sipradi Autoparts to ...</td>\n",
       "      <td>https://www.sharesansar.com/newsdetail/nabil-b...</td>\n",
       "      <td>Nabil Bank has inked an agreement with Sipradi...</td>\n",
       "      <td>0.091667</td>\n",
       "      <td>0.616667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2023-09-11</td>\n",
       "      <td>Commercial Banks; Too Pricey on your Pockets? ...</td>\n",
       "      <td>https://www.sharesansar.com/newsdetail/commerc...</td>\n",
       "      <td>As the stock market experiences continuous dec...</td>\n",
       "      <td>0.099123</td>\n",
       "      <td>0.438421</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date                                            Heading  \\\n",
       "0  2023-09-26  Banks and Finance Companies Report Impressive ...   \n",
       "1  2023-09-18  Nabil Bank inaugurates Namche Branch in Solukh...   \n",
       "2  2023-09-18  Commercial Banks in Ashwin Unveil Interest Rat...   \n",
       "3  2023-09-12  Nabil Bank partners with Sipradi Autoparts to ...   \n",
       "4  2023-09-11  Commercial Banks; Too Pricey on your Pockets? ...   \n",
       "\n",
       "                                      Link to detail  \\\n",
       "0  https://www.sharesansar.com/newsdetail/banks-a...   \n",
       "1  https://www.sharesansar.com/newsdetail/nabil-b...   \n",
       "2  https://www.sharesansar.com/newsdetail/commerc...   \n",
       "3  https://www.sharesansar.com/newsdetail/nabil-b...   \n",
       "4  https://www.sharesansar.com/newsdetail/commerc...   \n",
       "\n",
       "                                         Detail News  polarity  subjectivity  \n",
       "0  Nepal Rastra Bank has unveiled recent statisti...  0.100940      0.227429  \n",
       "1  Nabil Bank, a leading banking institution, has...  0.202564      0.410256  \n",
       "2  Commercial banks have recently announced their...  0.007362      0.271507  \n",
       "3  Nabil Bank has inked an agreement with Sipradi...  0.091667      0.616667  \n",
       "4  As the stock market experiences continuous dec...  0.099123      0.438421  "
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "3703234a-6bfc-4058-892d-ecc70a10a435",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>polarity</th>\n",
       "      <th>subjectivity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>780.000000</td>\n",
       "      <td>780.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.038859</td>\n",
       "      <td>0.192388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.094732</td>\n",
       "      <td>0.188457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-0.342121</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.199455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.065952</td>\n",
       "      <td>0.344633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.800000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         polarity  subjectivity\n",
       "count  780.000000    780.000000\n",
       "mean     0.038859      0.192388\n",
       "std      0.094732      0.188457\n",
       "min     -0.342121      0.000000\n",
       "25%      0.000000      0.000000\n",
       "50%      0.000000      0.199455\n",
       "75%      0.065952      0.344633\n",
       "max      0.800000      1.000000"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "ee418787-d9e3-431d-8f05-e469112fc30d",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df.to_csv(\"financial_news_with_sentiments.csv\",index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
